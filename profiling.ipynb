{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user_id                                          user_post\n",
      "0  S1NCL41R  Here is what I don’t get with this advice.  Wi...\n",
      "1  S1NCL41R  from a financial standpoint, buy an older hond...\n",
      "2  S1NCL41R  Correct.  The point is that he can’t take bene...\n",
      "3  S1NCL41R  The provider can provide you a hardship waiver...\n",
      "4  S1NCL41R  might i suggest that a key to this problem act...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "file_path = 'Reddit_data_train.json'  # Replace with the actual file path\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Flatten the JSON data into a list of dictionaries\n",
    "flattened_data = []\n",
    "for user_id, posts in data.items():\n",
    "    for post in posts:\n",
    "        flattened_data.append({'user_id': user_id, 'user_post': post['text']})\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('user_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Topic modelling, finding key topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/profiling.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Generate topics using OpenAI's language model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m topic_modeling_input \u001b[39m=\u001b[39m topic_modeling_prompt\u001b[39m.\u001b[39mformat(post\u001b[39m=\u001b[39mpost)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m topic_modeling_response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgpt-3.5-turbo-1106\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     prompt\u001b[39m=\u001b[39;49mtopic_modeling_input,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     n\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,  \u001b[39m# Generate up to 10 topics\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X11sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X11sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X11sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X11sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m generated_topics \u001b[39m=\u001b[39m topic_modeling_response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X11sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Verify topics using the WikipediaRetriever\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_args: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set up OpenAI API credentials\n",
    "openai.api_key = 'api_key'\n",
    "\n",
    "# Load the dataset with user IDs and posts\n",
    "data = pd.read_csv('user_data.csv')\n",
    "\n",
    "# Initialize the WikipediaRetriever\n",
    "retriever = WikipediaRetriever()\n",
    "\n",
    "# Define the prompt for topic modeling\n",
    "topic_modeling_prompt = \"\"\"\n",
    "Given the following user post:\n",
    "\"{post}\"\n",
    "\n",
    "Generate up to 10 key topics related to the post.\n",
    "\"\"\"\n",
    "\n",
    "# Define the prompt for user profiling\n",
    "user_profiling_prompt = \"\"\"\n",
    "Given the following user post:\n",
    "\"{post}\"\n",
    "\n",
    "Based on the generated topics:\n",
    "{topics}\n",
    "\n",
    "Assign a topic score to the user for each topic.\n",
    "\"\"\"\n",
    "\n",
    "# Perform topic modeling and user profiling for each user\n",
    "user_profiles = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    user_post = row['post']\n",
    "    \n",
    "    # Generate topics using OpenAI's language model\n",
    "    topic_modeling_input = topic_modeling_prompt.format(post=post)\n",
    "    topic_modeling_response = openai.Completion.create(\n",
    "        engine='gpt-3.5-turbo-1106',\n",
    "        prompt=topic_modeling_input,\n",
    "        max_tokens=100,\n",
    "        n=10,  # Generate up to 10 topics\n",
    "        stop=None,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    generated_topics = topic_modeling_response.choices[0].text.strip().split('\\n')\n",
    "    \n",
    "    # Verify topics using the WikipediaRetriever\n",
    "    verified_topics = []\n",
    "    for topic in generated_topics:\n",
    "        docs = retriever.get_relevant_documents(query=topic)\n",
    "        if docs:\n",
    "            verified_topics.append(topic)\n",
    "    \n",
    "    # Perform user profiling using the verified topics\n",
    "    user_profiling_input = user_profiling_prompt.format(post=user_post, topics='\\n'.join(verified_topics))\n",
    "    user_profiling_response = openai.Completion.create(\n",
    "        engine='gpt-3.5-turbo-1106',\n",
    "        prompt=user_profiling_input,\n",
    "        max_tokens=100,\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    topic_scores = user_profiling_response.choices[0].text.strip().split('\\n')\n",
    "    \n",
    "    # Create the user profile\n",
    "    user_profile = {\n",
    "        'user_id': user_id,\n",
    "        'topics': verified_topics,\n",
    "        'topic_scores': topic_scores\n",
    "    }\n",
    "    \n",
    "    user_profiles.append(user_profile)\n",
    "\n",
    "# Convert user profiles to DataFrame\n",
    "user_profiles_df = pd.DataFrame(user_profiles)\n",
    "\n",
    "# Save identified topics to a CSV file\n",
    "identified_topics_df = pd.DataFrame({'Identified Topics': user_profiles_df['topics'].explode().unique()})\n",
    "identified_topics_df.to_csv('identified_topics.csv', index=False)\n",
    "\n",
    "# Save user profiles to a CSV file\n",
    "user_profiles_df.to_csv('user_profiles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/profiling.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X21sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Generate topics using OpenAI's language model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X21sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m topic_modeling_input \u001b[39m=\u001b[39m topic_modeling_prompt\u001b[39m.\u001b[39mformat(post\u001b[39m=\u001b[39muser_post)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X21sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m topic_modeling_response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X21sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X21sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X21sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mYou are a helpful assistant that generates topics based on user posts.\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X21sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: topic_modeling_input},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X21sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X21sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X21sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X21sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m generated_topics \u001b[39m=\u001b[39m topic_modeling_response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X21sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Verify topics using the WikipediaRetriever\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_args: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set up OpenAI API credentials\n",
    "openai.api_key = api_key  # Use the API key loaded from environment variables\n",
    "\n",
    "# Load the dataset with user IDs and posts\n",
    "data = pd.read_csv('user_data.csv')\n",
    "\n",
    "# Initialize the WikipediaRetriever\n",
    "retriever = WikipediaRetriever()\n",
    "\n",
    "# Define the prompt for topic modeling\n",
    "topic_modeling_prompt = \"\"\"\n",
    "Given the following user post:\n",
    "\"{post}\"\n",
    "\n",
    "Generate up to 10 key topics related to the post.\n",
    "\"\"\"\n",
    "\n",
    "# Define the prompt for user profiling\n",
    "user_profiling_prompt = \"\"\"\n",
    "Given the following user post:\n",
    "\"{post}\"\n",
    "\n",
    "Based on the generated topics:\n",
    "{topics}\n",
    "\n",
    "Assign a topic score to the user for each topic.\n",
    "\"\"\"\n",
    "\n",
    "# Perform topic modeling and user profiling for each user\n",
    "user_profiles = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    user_post = row['user_post']\n",
    "    \n",
    "    # Generate topics using OpenAI's language model\n",
    "    topic_modeling_input = topic_modeling_prompt.format(post=user_post)\n",
    "    topic_modeling_response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates topics based on user posts.\"},\n",
    "            {\"role\": \"user\", \"content\": topic_modeling_input},\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    generated_topics = topic_modeling_response.choices[0].message[\"content\"].strip().split('\\n')\n",
    "    \n",
    "    # Verify topics using the WikipediaRetriever\n",
    "    verified_topics = []\n",
    "    for topic in generated_topics:\n",
    "        docs = retriever.get_relevant_documents(query=topic)\n",
    "        if docs:\n",
    "            verified_topics.append(topic)\n",
    "    \n",
    "    # Perform user profiling using the verified topics\n",
    "    user_profiling_input = user_profiling_prompt.format(post=user_post, topics='\\n'.join(verified_topics))\n",
    "    user_profiling_response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that assigns topic scores to user posts.\"},\n",
    "            {\"role\": \"user\", \"content\": user_profiling_input},\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    topic_scores = user_profiling_response.choices[0].message[\"content\"].strip().split('\\n')\n",
    "    \n",
    "    # Create the user profile\n",
    "    user_profile = {\n",
    "        'user_id': user_id,\n",
    "        'topics': verified_topics,\n",
    "        'topic_scores': topic_scores\n",
    "    }\n",
    "    \n",
    "    user_profiles.append(user_profile)\n",
    "\n",
    "# Convert user profiles to DataFrame\n",
    "user_profiles_df = pd.DataFrame(user_profiles)\n",
    "\n",
    "# Save identified topics to a CSV file\n",
    "identified_topics_df = pd.DataFrame({'Identified Topics': user_profiles_df['topics'].explode().unique()})\n",
    "identified_topics_df.to_csv('identified_topics.csv', index=False)\n",
    "\n",
    "# Save user profiles to a CSV file\n",
    "user_profiles_df.to_csv('user_profiles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/profiling.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X22sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m verified_topics \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X22sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m topic \u001b[39min\u001b[39;00m generated_topics:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X22sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     docs \u001b[39m=\u001b[39m retriever\u001b[39m.\u001b[39;49mget_relevant_documents(query\u001b[39m=\u001b[39;49mtopic)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X22sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39mif\u001b[39;00m docs:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kandari/Workspace/PLACEMENT/TASKS/farmwiseai%20-%20firstroundassignments/NLP2-farmwise-task/profiling.ipynb#X22sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         verified_topics\u001b[39m.\u001b[39mappend(topic)\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/langchain_core/retrievers.py:217\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m _kwargs \u001b[39m=\u001b[39m kwargs \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expects_other_args \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 217\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_relevant_documents(\n\u001b[1;32m    218\u001b[0m         query, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    220\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_relevant_documents(query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/langchain_community/retrievers/wikipedia.py:20\u001b[0m, in \u001b[0;36mWikipediaRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_relevant_documents\u001b[39m(\n\u001b[1;32m     18\u001b[0m     \u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m     19\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload(query\u001b[39m=\u001b[39;49mquery)\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/langchain_community/utilities/wikipedia.py:114\u001b[0m, in \u001b[0;36mWikipediaAPIWrapper.load\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m page_title \u001b[39min\u001b[39;00m page_titles[: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtop_k_results]:\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m wiki_page \u001b[39m:=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetch_page(page_title):\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mif\u001b[39;00m doc \u001b[39m:=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_page_to_document(page_title, wiki_page):\n\u001b[1;32m    115\u001b[0m             docs\u001b[39m.\u001b[39mappend(doc)\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/langchain_community/utilities/wikipedia.py:65\u001b[0m, in \u001b[0;36mWikipediaAPIWrapper._page_to_document\u001b[0;34m(self, page_title, wiki_page)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_page_to_document\u001b[39m(\u001b[39mself\u001b[39m, page_title: \u001b[39mstr\u001b[39m, wiki_page: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Document:\n\u001b[1;32m     63\u001b[0m     main_meta \u001b[39m=\u001b[39m {\n\u001b[1;32m     64\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m: page_title,\n\u001b[0;32m---> 65\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m\"\u001b[39m: wiki_page\u001b[39m.\u001b[39;49msummary,\n\u001b[1;32m     66\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: wiki_page\u001b[39m.\u001b[39murl,\n\u001b[1;32m     67\u001b[0m     }\n\u001b[1;32m     68\u001b[0m     add_meta \u001b[39m=\u001b[39m (\n\u001b[1;32m     69\u001b[0m         {\n\u001b[1;32m     70\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcategories\u001b[39m\u001b[39m\"\u001b[39m: wiki_page\u001b[39m.\u001b[39mcategories,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m     81\u001b[0m     )\n\u001b[1;32m     82\u001b[0m     doc \u001b[39m=\u001b[39m Document(\n\u001b[1;32m     83\u001b[0m         page_content\u001b[39m=\u001b[39mwiki_page\u001b[39m.\u001b[39mcontent[: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc_content_chars_max],\n\u001b[1;32m     84\u001b[0m         metadata\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m         },\n\u001b[1;32m     88\u001b[0m     )\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/wikipedia/wikipedia.py:530\u001b[0m, in \u001b[0;36mWikipediaPage.summary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    528\u001b[0m      query_params[\u001b[39m'\u001b[39m\u001b[39mpageids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpageid\n\u001b[0;32m--> 530\u001b[0m   request \u001b[39m=\u001b[39m _wiki_request(query_params)\n\u001b[1;32m    531\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary \u001b[39m=\u001b[39m request[\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mpages\u001b[39m\u001b[39m'\u001b[39m][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpageid][\u001b[39m'\u001b[39m\u001b[39mextract\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    533\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/wikipedia/wikipedia.py:737\u001b[0m, in \u001b[0;36m_wiki_request\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    734\u001b[0m   wait_time \u001b[39m=\u001b[39m (RATE_LIMIT_LAST_CALL \u001b[39m+\u001b[39m RATE_LIMIT_MIN_WAIT) \u001b[39m-\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m    735\u001b[0m   time\u001b[39m.\u001b[39msleep(\u001b[39mint\u001b[39m(wait_time\u001b[39m.\u001b[39mtotal_seconds()))\n\u001b[0;32m--> 737\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(API_URL, params\u001b[39m=\u001b[39;49mparams, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    739\u001b[0m \u001b[39mif\u001b[39;00m RATE_LIMIT:\n\u001b[1;32m    740\u001b[0m   RATE_LIMIT_LAST_CALL \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/requests/sessions.py:725\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    723\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 725\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;49;00m resp \u001b[39min\u001b[39;49;00m gen]\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/requests/sessions.py:725\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    723\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 725\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;49;00m resp \u001b[39min\u001b[39;49;00m gen]\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/requests/sessions.py:266\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    267\u001b[0m         req,\n\u001b[1;32m    268\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    269\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    270\u001b[0m         verify\u001b[39m=\u001b[39;49mverify,\n\u001b[1;32m    271\u001b[0m         cert\u001b[39m=\u001b[39;49mcert,\n\u001b[1;32m    272\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    273\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49madapter_kwargs,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     extract_cookies_to_jar(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcookies, prepared_request, resp\u001b[39m.\u001b[39mraw)\n\u001b[1;32m    279\u001b[0m     \u001b[39m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Workspace/PLACEMENT/TASKS/farmwiseai - firstroundassignments/NLP2-farmwise-task/venv/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1312\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the dataset with user IDs and posts\n",
    "data = pd.read_csv('user_data.csv')\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Define the prompt for topic modeling\n",
    "topic_modeling_prompt = \"\"\"\n",
    "Given the following user post:\n",
    "\"{post}\"\n",
    "\n",
    "Generate up to 10 key topics related to the post.\n",
    "\"\"\"\n",
    "\n",
    "# Define the prompt for user profiling\n",
    "user_profiling_prompt = \"\"\"\n",
    "Given the following user post:\n",
    "\"{post}\"\n",
    "\n",
    "Based on the generated topics:\n",
    "{topics}\n",
    "\n",
    "Assign a topic score to the user for each topic.\n",
    "\"\"\"\n",
    "\n",
    "# Perform topic modeling and user profiling for each user\n",
    "user_profiles = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    user_post = row['user_post']\n",
    "    \n",
    "    # Generate topics using OpenAI's GPT-3.5 model\n",
    "    topic_modeling_input = topic_modeling_prompt.format(post=user_post)\n",
    "    topic_modeling_response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": topic_modeling_input},\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",  # Use the GPT-3.5 model\n",
    "        temperature=0\n",
    "    )\n",
    "    generated_topics = topic_modeling_response.choices[0].message.content.strip().split('\\n')\n",
    "    \n",
    "    # Verify topics using the WikipediaRetriever\n",
    "    verified_topics = []\n",
    "    for topic in generated_topics:\n",
    "        docs = retriever.get_relevant_documents(query=topic)\n",
    "        if docs:\n",
    "            verified_topics.append(topic)\n",
    "    \n",
    "    # Perform user profiling using the verified topics\n",
    "    user_profiling_input = user_profiling_prompt.format(post=user_post, topics='\\n'.join(verified_topics))\n",
    "    user_profiling_response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_profiling_input},\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",  # Use the GPT-3.5 model\n",
    "        temperature=0,\n",
    "        timeout=60\n",
    "    )\n",
    "    topic_scores = user_profiling_response.choices[0].message.content.strip().split('\\n')\n",
    "    \n",
    "    # Create the user profile\n",
    "    user_profile = {\n",
    "        'user_id': user_id,\n",
    "        'topics': verified_topics,\n",
    "        'topic_scores': topic_scores\n",
    "    }\n",
    "    \n",
    "    user_profiles.append(user_profile)\n",
    "\n",
    "# Convert user profiles to DataFrame\n",
    "user_profiles_df = pd.DataFrame(user_profiles)\n",
    "\n",
    "# Save identified topics to a CSV file\n",
    "identified_topics_df = pd.DataFrame({'Identified Topics': user_profiles_df['topics'].explode().unique()})\n",
    "identified_topics_df.to_csv('identified_topics.csv', index=False)\n",
    "\n",
    "# Save user profiles to a CSV file\n",
    "user_profiles_df.to_csv('user_profiles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import concurrent.futures\n",
    "\n",
    "# Load the dataset with user IDs and posts\n",
    "data = pd.read_csv('user_data2.csv')\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Define the prompt for topic modeling\n",
    "topic_modeling_prompt = \"\"\"\n",
    "Given the following user post:\n",
    "\"{post}\"\n",
    "\n",
    "Generate up to 10 key topics related to the post.\n",
    "\"\"\"\n",
    "\n",
    "# Define the prompt for user profiling\n",
    "user_profiling_prompt = \"\"\"\n",
    "Given the following user post:\n",
    "\"{post}\"\n",
    "\n",
    "Based on the generated topics:\n",
    "{topics}\n",
    "\n",
    "Assign a topic score to the user for each topic.\n",
    "\"\"\"\n",
    "\n",
    "# Function to create batches from the dataset\n",
    "def create_batches(dataframe, batch_size):\n",
    "    for i in range(0, len(dataframe), batch_size):\n",
    "        yield dataframe.iloc[i:i + batch_size]\n",
    "\n",
    "# Set batch size\n",
    "BATCH_SIZE = 10  # Adjust based on your requirements and API limitations\n",
    "\n",
    "# Function to process a batch of user posts\n",
    "def process_batch(batch):\n",
    "    batch_results = []\n",
    "    for _, row in batch.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        user_post = row['user_post']\n",
    "        \n",
    "        # Generate topics using OpenAI's GPT-3.5 model\n",
    "        topic_modeling_input = topic_modeling_prompt.format(post=user_post)\n",
    "        topic_modeling_response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": topic_modeling_input},\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",  # Use the GPT-3.5 model\n",
    "            temperature=0\n",
    "        )\n",
    "        generated_topics = topic_modeling_response.choices[0].message.content.strip().split('\\n')\n",
    "        \n",
    "        # Perform user profiling using the generated topics\n",
    "        user_profiling_input = user_profiling_prompt.format(post=user_post, topics='\\n'.join(generated_topics))\n",
    "        user_profiling_response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": user_profiling_input},\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",  # Use the GPT-3.5 model\n",
    "            temperature=0,\n",
    "            timeout=60\n",
    "        )\n",
    "        topic_scores = user_profiling_response.choices[0].message.content.strip().split('\\n')\n",
    "        \n",
    "        # Create the user profile\n",
    "        user_profile = {\n",
    "            'user_id': user_id,\n",
    "            'topics': generated_topics,\n",
    "            'topic_scores': topic_scores\n",
    "        }\n",
    "        batch_results.append(user_profile)\n",
    "    return batch_results\n",
    "\n",
    "# Perform topic modeling and user profiling in parallel for batches\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Create batches\n",
    "    batches = list(create_batches(data, BATCH_SIZE))\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    results = executor.map(process_batch, batches)\n",
    "\n",
    "# Flatten the results from batches\n",
    "user_profiles = [profile for batch in results for profile in batch]\n",
    "\n",
    "# Convert user profiles to DataFrame\n",
    "user_profiles_df = pd.DataFrame(user_profiles)\n",
    "\n",
    "# Save identified topics to a CSV file\n",
    "identified_topics_df = pd.DataFrame({'Identified Topics': user_profiles_df['topics'].explode().unique()})\n",
    "identified_topics_df.to_csv('identified_topics.csv', index=False)\n",
    "\n",
    "# Save user profiles to a CSV file\n",
    "user_profiles_df.to_csv('user_profiles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import concurrent.futures\n",
    "\n",
    "# Load the dataset with user IDs and posts\n",
    "data = pd.read_csv('user_data2.csv')\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Define the prompt for topic modeling\n",
    "topic_modeling_prompt = \"\"\"\n",
    "Given the following user post:\n",
    "\"{post}\"\n",
    "\n",
    "Generate up to 10 key topics related to the post.\n",
    "\"\"\"\n",
    "\n",
    "# Define the prompt for user profiling\n",
    "user_profiling_prompt = \"\"\"\n",
    "Given the following user post:\n",
    "\"{post}\"\n",
    "\n",
    "Based on the generated topics:\n",
    "{topics}\n",
    "\n",
    "Assign a topic score to the user for each topic.\n",
    "\"\"\"\n",
    "\n",
    "# Function to create batches from the dataset\n",
    "def create_batches(dataframe, batch_size):\n",
    "    for i in range(0, len(dataframe), batch_size):\n",
    "        yield dataframe.iloc[i:i + batch_size]\n",
    "\n",
    "# Set batch size\n",
    "BATCH_SIZE = 10  # Adjust based on your requirements and API limitations\n",
    "\n",
    "# Function to process a batch of user posts and collect topics\n",
    "def process_batch(batch):\n",
    "    batch_results = []\n",
    "    all_topics = []  # Collect all topics from each user post\n",
    "    for _, row in batch.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        user_post = row['user_post']\n",
    "        \n",
    "        # Generate topics using OpenAI's GPT-3.5 model\n",
    "        topic_modeling_input = topic_modeling_prompt.format(post=user_post)\n",
    "        topic_modeling_response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": topic_modeling_input},\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",  # Use the GPT-3.5 model\n",
    "            temperature=0\n",
    "        )\n",
    "        generated_topics = topic_modeling_response.choices[0].message.content.strip().split('\\n')\n",
    "        all_topics.extend(generated_topics)  # Add generated topics to the all_topics list\n",
    "        \n",
    "        # Perform user profiling using the generated topics\n",
    "        user_profiling_input = user_profiling_prompt.format(post=user_post, topics='\\n'.join(generated_topics))\n",
    "        user_profiling_response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": user_profiling_input},\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",  # Use the GPT-3.5 model\n",
    "            temperature=0,\n",
    "            timeout=60\n",
    "        )\n",
    "        topic_scores = user_profiling_response.choices[0].message.content.strip().split('\\n')\n",
    "        \n",
    "        # Create the user profile\n",
    "        user_profile = {\n",
    "            'user_id': user_id,\n",
    "            'topics': generated_topics,\n",
    "            'topic_scores': topic_scores\n",
    "        }\n",
    "        batch_results.append(user_profile)\n",
    "    return batch_results, all_topics\n",
    "\n",
    "# Perform topic modeling and user profiling in parallel for batches\n",
    "all_topics_collected = []  # List to collect topics from all batches\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Create batches\n",
    "    batches = list(create_batches(data, BATCH_SIZE))\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    batch_results = executor.map(process_batch, batches)\n",
    "    \n",
    "    # Flatten the results from batches and collect topics\n",
    "    user_profiles = []\n",
    "    for batch_result, batch_topics in batch_results:\n",
    "        user_profiles.extend(batch_result)\n",
    "        all_topics_collected.extend(batch_topics)\n",
    "\n",
    "# Analyze and select the top 20 topics\n",
    "top_20_topics = pd.Series(all_topics_collected).value_counts().head(20).index.tolist()\n",
    "\n",
    "# Convert user profiles to DataFrame\n",
    "user_profiles_df = pd.DataFrame(user_profiles)\n",
    "\n",
    "# Save top 20 identified topics to a CSV file\n",
    "identified_topics_df = pd.DataFrame({'Identified Topics': top_20_topics})\n",
    "identified_topics_df.to_csv('identified_topics_v2.csv', index=False)\n",
    "\n",
    "# Save user profiles to a CSV file\n",
    "user_profiles_df.to_csv('user_profiles_v2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import concurrent.futures\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset with user IDs and posts\n",
    "data = pd.read_csv('user_data2.csv')\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Define the prompt for topic modeling\n",
    "topic_modeling_prompt = \"\"\"\n",
    "Given the following user post:\n",
    "\"{post}\"\n",
    "\n",
    "Generate key topics related to the post.\n",
    "\"\"\"\n",
    "\n",
    "# Set batch size\n",
    "BATCH_SIZE = 10  # Adjust based on your requirements and API limitations\n",
    "\n",
    "# Function to create batches from the dataset\n",
    "def create_batches(dataframe, batch_size):\n",
    "    for i in range(0, len(dataframe), batch_size):\n",
    "        yield dataframe.iloc[i:i + batch_size]\n",
    "\n",
    "# Function to process a batch of user posts and collect topics\n",
    "def process_batch(batch):\n",
    "    user_topics = {}  # Dictionary to store topics for each user\n",
    "    post_profiles = []\n",
    "    all_topics = []   # List to collect all topics\n",
    "    for _, row in batch.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        user_post = row['user_post']\n",
    "\n",
    "        # Generate topics using OpenAI's GPT-3.5 model\n",
    "        topic_modeling_input = topic_modeling_prompt.format(post=user_post)\n",
    "        topic_modeling_response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": topic_modeling_input},\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0\n",
    "        )\n",
    "        generated_topics = topic_modeling_response.choices[0].message.content.strip().split('\\n')[:10]  # Limit to 10 topics per post\n",
    "        single_word_topics = [topic for topic in generated_topics if len(topic.split()) == 1]  # Filter for single-word topics\n",
    "\n",
    "        # Aggregate topics for each user\n",
    "        if user_id not in user_topics:\n",
    "            user_topics[user_id] = []\n",
    "        user_topics[user_id].extend(single_word_topics)\n",
    "\n",
    "        # Create post profile with associated topics\n",
    "        post_profile = {\n",
    "            'user_id': user_id,\n",
    "            'user_post': user_post,\n",
    "            'topics': single_word_topics\n",
    "        }\n",
    "        post_profiles.append(post_profile)\n",
    "\n",
    "        # Collect all single-word topics\n",
    "        all_topics.extend(single_word_topics)\n",
    "\n",
    "    return user_topics, post_profiles, all_topics\n",
    "\n",
    "# Perform topic modeling in parallel for batches\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Create batches\n",
    "    batches = list(create_batches(data, BATCH_SIZE))\n",
    "\n",
    "    # Process batches in parallel\n",
    "    batch_results = executor.map(process_batch, batches)\n",
    "\n",
    "    # Flatten the results from batches and aggregate user topics\n",
    "    aggregated_user_topics = {}\n",
    "    all_post_profiles = []\n",
    "    all_topics_collected = []\n",
    "    for user_topics, post_profiles, all_topics in batch_results:\n",
    "        for user_id, topics in user_topics.items():\n",
    "            if user_id not in aggregated_user_topics:\n",
    "                aggregated_user_topics[user_id] = Counter()\n",
    "            aggregated_user_topics[user_id].update(topics)\n",
    "        all_post_profiles.extend(post_profiles)\n",
    "        all_topics_collected.extend(all_topics)\n",
    "\n",
    "# Create user profiles with the most frequent single-word topics\n",
    "user_profiles = []\n",
    "for user_id, topics_counter in aggregated_user_topics.items():\n",
    "    top_topics = [topic for topic, _ in topics_counter.most_common(10)]  # Get the top 10 topics\n",
    "    user_profiles.append({'user_id': user_id, 'topics': top_topics})\n",
    "\n",
    "# Identify the top 20 single-word topics used across all posts\n",
    "top_20_topics = pd.Series(all_topics_collected).value_counts().head(20).index.tolist()\n",
    "\n",
    "# Convert to DataFrames\n",
    "user_profiles_df = pd.DataFrame(user_profiles)\n",
    "post_profiles_df = pd.DataFrame(all_post_profiles)\n",
    "top_20_topics_df = pd.DataFrame({'Top 20 Topics': top_20_topics})\n",
    "\n",
    "# Save user profiles to a CSV file\n",
    "user_profiles_df.to_csv('user_profiles_merged_v3.csv', index=False)\n",
    "\n",
    "# Save post profiles with topics to a CSV file\n",
    "post_profiles_df.to_csv('post_profiles_merged_v3.csv', index=False)\n",
    "\n",
    "# Save the top 20 topics to a CSV file\n",
    "top_20_topics_df.to_csv('top_20_topics_merged.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
